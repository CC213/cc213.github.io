---
layout: post
title: Lucene分词器
categories: [搜索引擎, Lucene, 分词器]
---

## 开篇

分词，简而言之，就是把一段长的`文本`按照某种（某些）`约定`切分成更短的文本。

分词器，就是“切分”操作的`执行者`。

本文将从“文本”、“约定”以及“执行者”角度，简要分析Lucene分词器的`工作方式`。本文不讨论分词的算法（感兴趣的同学可以用[谷歌学术搜索](http://scholar.google.com/)或[微软学术搜索](http://academic.research.microsoft.com/)去查找相关资料），也不讨论分词器的具体使用方法（有这方面需求的同学，可以查看[Lucene官方文档](http://lucene.apache.org/core/3_5_0/api/all/index.html)）。本文基于Lucene3.5版本的代码进行分析。

## Lucene分词器工作方式

Lucene分词器的工作方式可以用下图表示：

![](http://cc213.github.io/images/Lucene%E5%88%86%E8%AF%8D%E5%99%A8/workflow.png "图1 - Lucene分词器工作方式")

- `文本`

	分词，首先要有被切分的文本，在图中就是绿色部分对应的文字“A TokenStream enumerates the sequence of tokens, either from Fields of a Document or from query text”，在Lucene的API实现中，以Reader参数传入。

- `约定` + `执行者`

	有了待切分的`文本`以后，接下来要关心的问题就是按照什么样的`约定`（对应图中的蓝色部分，`约定`的另一个高大上名字就是算法）来进行切分操作，在Lucene中与此相关的类是[TokenStream](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/TokenStream.html)；与此同时，TokenStream也是切分操作的`执行者`。在它的API中，有一个非常重要的方法[incrementToken()](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/TokenStream.html#incrementToken())：

	```Java
	public abstract boolean incrementToken() throws IOException;
	```

	Lucene官方文档是这样描述incrementToken()方法的：“Consumers (i.e., IndexWriter) use this method to advance the stream to the next token”。因此，所有分词的`约定`都是在这个方法中实现的。Lucene在索引和查询的时候，通过这个方法遍历token；当没有可以再访问的token时，方法返回false。

	在分词的过程中，TokenStream需要记录下来一些属性（对应图中的紫色部分，在Lucene中通过[Attribute](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/util/Attribute.html)实现）。这些属性可以用来获取token的文本（[CharTermAttribute](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/tokenattributes/CharTermAttribute.html)），或者作为分词`约定`的参数来影响下一个token的切分操作([OffsetAttribute](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/tokenattributes/OffsetAttribute.html))。Lucene将这些属性集中放在[AttributeSource](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/util/AttributeSource.html)中，TokenStream通过继承AttributeSource来读取和更新这些属性。

	TokenStream可以由[Analyzer](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/Analyzer.html)来创建：

	```Java
	public abstract TokenStream tokenStream(String fieldName, Reader reader);
	```

## CharTokenizer分析

incrementToken()方法是分词器的核心。接下来，我们简要分析一下[CharTokenizer](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/CharTokenizer.html)的这个方法的[源码](http://cc213.github.io/files/Lucene%E5%88%86%E8%AF%8D%E5%99%A8/CharTokenizer.java)：

### 源码：
```Java
  @Override
  public final boolean incrementToken() throws IOException {
	clearAttributes();
	if(useOldAPI) // TODO remove this in LUCENE 4.0
	  return incrementTokenOld();
	int length = 0;
	int start = -1; // this variable is always initialized
	char[] buffer = termAtt.buffer();
	while (true) {
	  if (bufferIndex >= dataLen) {
		offset += dataLen;
		if(!charUtils.fill(ioBuffer, input)) { // read supplementary char aware with CharacterUtils
		  dataLen = 0; // so next offset += dataLen won't decrement offset
		  if (length > 0) {
			break;
		  } else {
			finalOffset = correctOffset(offset);
			return false;
		  }
		}
		dataLen = ioBuffer.getLength();
		bufferIndex = 0;
	  }
	  // use CharacterUtils here to support < 3.1 UTF-16 code unit behavior if the char based methods are gone
	  final int c = charUtils.codePointAt(ioBuffer.getBuffer(), bufferIndex);
	  bufferIndex += Character.charCount(c);

	  if (isTokenChar(c)) {               // if it's a token char
		if (length == 0) {                // start of token
		  assert start == -1;
		  start = offset + bufferIndex - 1;
		} else if (length >= buffer.length-1) { // check if a supplementary could run out of bounds
		  buffer = termAtt.resizeBuffer(2+length); // make sure a supplementary fits in the buffer
		}
		length += Character.toChars(normalize(c), buffer, length); // buffer it, normalized
		if (length >= MAX_WORD_LEN) // buffer overflow! make sure to check for >= surrogate pair could break == test
		  break;
	  } else if (length > 0)             // at non-Letter w/ chars
		break;                           // return 'em
	}

	termAtt.setLength(length);
	assert start != -1;
	offsetAtt.setOffset(correctOffset(start), finalOffset = correctOffset(start+length));
	return true;
	
  }
```

### 相关变量描述

1. CharTokenizer成员变量

input：Reader，从父类[Tokenizer](http://lucene.apache.org/core/3_5_0/api/all/org/apache/lucene/analysis/Tokenizer.html)继承，构造方法传入，待切分文本。

ioBuffer：CharacterBuffer，char[]的封装，从input中拷贝字符，所有切分的计算都在这上面进行。

termAtt：CharTermAttribute，存储切分的token的文本属性

bufferIndex：int，ioBuffer下次开始读取的位置

dataLen：int，ioBuffer中读入的字符数

offset：int，Reader缓冲区下次开始读取的位置

2. incrementToken()方法变量

length：int，token的长度

start：int，token在Reader缓冲区中的起始index

buffer：char[]，引用，termAtt的termBuffer。

### 方法流程描述

